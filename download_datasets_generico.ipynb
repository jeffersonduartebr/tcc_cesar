{
  "cells": [
    {
      "cell_type": "code",
<<<<<<< HEAD
      "execution_count": null,
=======
      "execution_count": 8,
>>>>>>> ec389c935dbfa3937d63784935b9a2cb8b7a813e
      "metadata": {
        "id": "wwDWkrHvGD5N"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import requests\n",
        "import json\n",
        "import datetime\n",
        "import concurrent.futures\n",
<<<<<<< HEAD
        "import swifter\n",
=======
        "#import swifter\n",
>>>>>>> ec389c935dbfa3937d63784935b9a2cb8b7a813e
        "import os"
      ]
    },
    {
      "cell_type": "code",
<<<<<<< HEAD
      "execution_count": null,
=======
      "execution_count": 9,
>>>>>>> ec389c935dbfa3937d63784935b9a2cb8b7a813e
      "metadata": {
        "id": "zgfVV605bUpu"
      },
      "outputs": [],
      "source": [
        "pd.set_option('display.max_columns', None)  # or 1000\n",
        "pd.set_option('display.max_rows', None)  # or 1000\n",
        "pd.set_option('display.max_colwidth', None)  # or 199"
      ]
    },
    {
      "cell_type": "code",
<<<<<<< HEAD
      "execution_count": null,
=======
      "execution_count": 10,
>>>>>>> ec389c935dbfa3937d63784935b9a2cb8b7a813e
      "metadata": {},
      "outputs": [],
      "source": [
        "def converte_data(data_str):\n",
<<<<<<< HEAD
        "    return pd.to_datetime(data_str).tz_convert('America/Sao_Paulo')\n",
=======
        "    return pd.to_datetime(data_str, errors='ignore').tz_convert('America/Sao_Paulo')\n",
>>>>>>> ec389c935dbfa3937d63784935b9a2cb8b7a813e
        "\n",
        "\n",
        "def gera_lista_assuntos(assuntos_do_df):\n",
        "    lst_assuntos=[]\n",
        "    for assunto in assuntos_do_df:\n",
        "        try:\n",
        "            lst_assuntos.append(assunto.get('nome'))\n",
        "        except:\n",
        "            lst_assuntos.append('')\n",
        "\n",
        "    return lst_assuntos\n",
        "\n",
        "\n",
        "def gera_lista_movimentos(movimentos):\n",
        "    lst_movimentos_final =[]\n",
        "    for movimento in movimentos:\n",
        "        codigo = movimento.get('codigo')\n",
        "        nome = movimento.get('nome')\n",
        "        data_hora = movimento.get('dataHora')\n",
        "        if data_hora:\n",
        "            data_hora = converte_data(data_hora)\n",
        "        lst_movimentos_final.append([ codigo, nome, data_hora])\n",
        "    return lst_movimentos_final\n",
        "\n",
        "def process_movimento(movimento):\n",
        "    codigo = movimento.get('codigo')\n",
        "    nome = movimento.get('nome')\n",
        "    data_hora = movimento.get('dataHora')\n",
        "    if data_hora:\n",
        "        data_hora = converte_data(data_hora)\n",
        "    return [codigo, nome, data_hora]\n",
        "\n",
        "def gera_lista_movimentos_multithread(movimentos):\n",
        "    lst_movimentos_final = []\n",
        "    with concurrent.futures.ThreadPoolExecutor() as executor:\n",
        "        results = list(executor.map(process_movimento, movimentos))\n",
        "        lst_movimentos_final.extend(results)\n",
        "    return lst_movimentos_final"
      ]
    },
    {
      "cell_type": "code",
<<<<<<< HEAD
      "execution_count": null,
=======
      "execution_count": 11,
>>>>>>> ec389c935dbfa3937d63784935b9a2cb8b7a813e
      "metadata": {
        "id": "fCRLqGiKGD5Q"
      },
      "outputs": [],
      "source": [
        "def lista_para_dataframe(dados_dict):\n",
        "  processos = []\n",
        "  for processo in dados_dict['hits']['hits']:\n",
        "    numero_processo = processo['_source']['numeroProcesso']\n",
        "    grau = processo['_source']['grau']\n",
        "    classe = processo['_source']['classe']['codigo']\n",
        "    try:\n",
        "      assuntos = processo['_source']['assuntos'] # Pode ter mais de um\n",
        "    except:\n",
        "      assuntos = []\n",
        "    data_ajuizamento = processo['_source']['dataAjuizamento']\n",
        "    ultima_atualizacao = processo['_source']['dataHoraUltimaAtualizacao']\n",
        "    #formato = processo['_source']['formato']['nome']\n",
        "    codigo = processo['_source']['orgaoJulgador']['codigo']\n",
        "    orgao_julgador = processo['_source']['orgaoJulgador']['nome']\n",
        "    municipio = processo['_source']['orgaoJulgador']['codigoMunicipioIBGE']\n",
        "    sort = processo['sort'][0]\n",
        "    try:\n",
        "      movimentos = processo['_source']['movimentos']\n",
        "    except:\n",
        "      movimentos = []\n",
        "\n",
        "    processos.append([numero_processo, classe, data_ajuizamento, ultima_atualizacao, \\\n",
        "                      codigo, orgao_julgador, municipio, grau, assuntos, movimentos, sort])\n",
        "\n",
        "  df = pd.DataFrame(processos, columns=['numero_processo', 'codigo_classe', 'data_ajuizamento', 'ultima_atualizacao', \\\n",
        "                        'codigo', 'orgao_julgador', 'municipio', 'grau', 'assuntos', 'movimentos', 'sort'])\n",
<<<<<<< HEAD
        "  df['movimentos'] = df['movimentos'].swifter.apply(gera_lista_movimentos_multithread)\n",
        "  df['assuntos'] = df['assuntos'].swifter.apply(gera_lista_assuntos)\n",
        "  #df['movimentos'] = df['movimentos'].swifter.apply(gera_lista_movimentos_multithread)\n",
        "  df['data_ajuizamento'] = df['data_ajuizamento'].swifter.apply(converte_data)\n",
        "  df['ultima_atualizacao'] = df['ultima_atualizacao'].swifter.apply(converte_data)\n",
        "  try:\n",
        "    df['movimentos']= df['movimentos'].swifter.apply(lambda x: sorted(x, key=lambda tup: tup[2], reverse=False))\n",
=======
        "  df['movimentos'] = df['movimentos'].apply(gera_lista_movimentos_multithread)\n",
        "  df['assuntos'] = df['assuntos'].apply(gera_lista_assuntos)\n",
        "  #df['movimentos'] = df['movimentos'].swifter.apply(gera_lista_movimentos_multithread)\n",
        "  try:\n",
        "    df['data_ajuizamento'] = df['data_ajuizamento'].apply(converte_data)\n",
        "    df['ultima_atualizacao'] = df['ultima_atualizacao'].apply(converte_data)\n",
        "    df['movimentos']= df['movimentos'].apply(lambda x: sorted(x, key=lambda tup: tup[2], reverse=False))\n",
>>>>>>> ec389c935dbfa3937d63784935b9a2cb8b7a813e
        "  except:\n",
        "    pass\n",
        "  return df"
      ]
    },
    {
      "cell_type": "code",
<<<<<<< HEAD
      "execution_count": null,
=======
      "execution_count": 12,
>>>>>>> ec389c935dbfa3937d63784935b9a2cb8b7a813e
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2L6SQFgbGD5O",
        "outputId": "b5955baf-6132-4b09-f181-3a68bc5ca34c"
      },
      "outputs": [],
      "source": [
        "def criar_dataset(tribunal, data, tamanho_consulta, grau='JE'):\n",
        "  match tribunal:\n",
        "    case 'TRF1':\n",
        "      tribunal = 'TRF1'\n",
        "      url = \"https://api-publica.datajud.cnj.jus.br/api_publica_trf1/_search\"\n",
        "    case 'TRF2':\n",
        "      tribunal = 'TRF2'\n",
        "      url = \"https://api-publica.datajud.cnj.jus.br/api_publica_trf2/_search\"    \n",
        "    case 'TRF3':\n",
        "      tribunal = 'TRF3'\n",
        "      url = \"https://api-publica.datajud.cnj.jus.br/api_publica_trf3/_search\"      \n",
        "    case 'TRF4':\n",
        "      tribunal = 'TRF4'\n",
        "      url = \"https://api-publica.datajud.cnj.jus.br/api_publica_trf4/_search\"  \n",
        "    case _:\n",
        "      tribunal = 'TRF4'\n",
        "      url = \"https://api-publica.datajud.cnj.jus.br/api_publica_trf4/_search\"          \n",
<<<<<<< HEAD
        "  df_tribunal = pd.DataFrame()\n",
=======
        "  df_tjrn = pd.DataFrame()\n",
>>>>>>> ec389c935dbfa3937d63784935b9a2cb8b7a813e
        "\n",
        "  api_key = \"APIKey cDZHYzlZa0JadVREZDJCendQbXY6SkJlTzNjLV9TRENyQk1RdnFKZGRQdw==\" # Chave pública\n",
        "  size = tamanho_consulta\n",
        "  data = data\n",
        "  grau = grau\n",
        "\n",
        "  payload = json.dumps(\n",
        "  {\n",
        "  \"size\": tamanho_consulta,\n",
        "  \"query\": {\n",
        "      \"bool\": {\n",
        "        \"must\": [\n",
        "            {\"match\": {\"tribunal\": tribunal}},\n",
        "            {\"match\": {\"grau\": grau}},            \n",
        "            {\"range\": {\"dataAjuizamento\": {\"gte\": data }}}\n",
        "        ]\n",
        "      }\n",
        "  },\n",
        "    \"sort\": [{\"@timestamp\": {\"order\": \"asc\"}}]\n",
        "  })\n",
        "\n",
        "  headers = {\n",
        "    'Authorization': api_key,\n",
        "    'Content-Type': 'application/json'\n",
        "  }\n",
        "\n",
        "  response = requests.request(\"POST\", url, headers=headers, data=payload)  # <Response [200]>\n",
        "  dados_dict = response.json() # <class 'dict'>\n",
        "  if len(dados_dict['hits']['hits']) < 5:\n",
        "    print(f'Parece que você digitou um código de Serventia errado. Confira novamente.')\n",
<<<<<<< HEAD
        "  df_tribunal = lista_para_dataframe(dados_dict)\n",
=======
        "  df_tjrn = lista_para_dataframe(dados_dict)\n",
>>>>>>> ec389c935dbfa3937d63784935b9a2cb8b7a813e
        "  numero_processos = size\n",
        "\n",
        "  while numero_processos == size:\n",
        "    numero_processos = len(dados_dict['hits']['hits'])\n",
        "    tamanho_dicionario_retornado = len(dados_dict['hits']['hits'])-1\n",
        "    if tamanho_dicionario_retornado < 1:\n",
        "      print(f'Tamanho do dicionário da página anterior: {tamanho_dicionario_retornado}')\n",
        "      continue\n",
        "    ultima_posicao_dicionario = dados_dict['hits']['hits'][(len(dados_dict['hits']['hits'])-1)]['sort'][0]\n",
        "    #print(f'Partindo da posição: {ultima_posicao_dicionario}')\n",
        "    payload = json.dumps(\n",
        "    {\n",
        "    \"size\": tamanho_consulta,\n",
        "    \"query\": {\n",
        "        \"bool\": {\n",
        "          \"must\": [\n",
        "            {\"match\": {\"tribunal\": tribunal}},\n",
        "            {\"match\": {\"grau\": grau}},               \n",
        "            {\"range\": {\"dataAjuizamento\": {\"gte\": data}}}\n",
        "            \n",
        "          ]\n",
        "        }\n",
        "    },\n",
        "      \"search_after\": [ ultima_posicao_dicionario ],\n",
        "      \"sort\": [{\"@timestamp\": {\"order\": \"asc\"}}]\n",
        "    })\n",
        "\n",
        "    headers = {\n",
        "      'Authorization': api_key,\n",
        "      'Content-Type': 'application/json'\n",
        "    }\n",
        "\n",
        "    response = requests.request(\"POST\", url, headers=headers, data=payload)  # <Response [200]>\n",
        "    dados_dict = response.json() # <class 'dict'>\n",
        "    numero_processos = len(dados_dict['hits']['hits'])\n",
        "    ultima_posicao_dicionario = dados_dict['hits']['hits'][(len(dados_dict['hits']['hits'])-1)]['sort']\n",
<<<<<<< HEAD
        "    df_tribunal = pd.concat([df_tribunal, lista_para_dataframe(dados_dict)])\n",
        "    df_tribunal = df_tribunal[df_tribunal['grau'] == grau]\n",
        "    tamanho_dataset = len(df_tribunal.index)\n",
=======
        "    df_tjrn = pd.concat([df_tjrn, lista_para_dataframe(dados_dict)])\n",
        "    df_tjrn = df_tjrn[df_tjrn['grau'] == grau]\n",
        "    tamanho_dataset = len(df_tjrn.index)\n",
>>>>>>> ec389c935dbfa3937d63784935b9a2cb8b7a813e
        "    ultima_data_ajuizamento = dados_dict['hits']['hits'][len(dados_dict['hits']['hits'])-1]['_source']['dataAjuizamento']\n",
        "    print(f'{datetime.datetime.now()}\\t Número de processos: {tamanho_dataset} \\t Data do último processo adicionado: {ultima_data_ajuizamento}' )\n",
        "    #if tamanho_dataset > 2000000:\n",
        "    #  break\n",
        "\n",
        "  try:\n",
        "    print(f'Número de processos incorporados: {tamanho_dataset}')\n",
        "  except:\n",
        "    print(f'Última página do dicionário veio vazia: {tribunal}')\n",
<<<<<<< HEAD
        "  #print(df_tribunal.head(1))\n",
        "  return df_tribunal"
=======
        "  #print(df_tjrn.head(1))\n",
        "  return df_tjrn"
>>>>>>> ec389c935dbfa3937d63784935b9a2cb8b7a813e
      ]
    },
    {
      "cell_type": "code",
<<<<<<< HEAD
      "execution_count": null,
=======
      "execution_count": 13,
>>>>>>> ec389c935dbfa3937d63784935b9a2cb8b7a813e
      "metadata": {},
      "outputs": [],
      "source": [
        "def salvar_dataset(df, orgao, grau , data):        \n",
        "    if not type(orgao) is str:\n",
        "        str(orgao)\n",
        "    orgao = orgao.replace(' ', '_')\n",
        "    nome_df = orgao + '_' + grau + '_' + data + '.csv'\n",
        "    nome_df = nome_df.replace(\" \", \"_\")\n",
        "    if os.path.isdir('./dados'):\n",
        "        os.chdir('dados')\n",
        "    df.to_csv(nome_df, sep=';', header=True, index=False, compression='zip')"
      ]
    },
    {
      "cell_type": "code",
<<<<<<< HEAD
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#lista_orgaos = ['2º JUIZADO ESPECIAL DA FAZENDA PÚBLICA', '3º JUIZADO ESPECIAL DA FAZENDA PÚBLICA', \n",
        "#                '5º JUIZADO ESPECIAL DA FAZENDA PÚBLICA']\n",
        "lista_tribunais = [ 'TRF4', 'TRF2', 'TRF3', 'TRF1']\n",
        "data = '2018-01-01'\n",
=======
      "execution_count": 14,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/tmp/ipykernel_99616/4177211250.py:2: FutureWarning: errors='ignore' is deprecated and will raise in a future version. Use to_datetime without passing `errors` and catch exceptions explicitly instead\n",
            "  return pd.to_datetime(data_str, errors='ignore').tz_convert('America/Sao_Paulo')\n"
          ]
        },
        {
          "ename": "IndexError",
          "evalue": "list index out of range",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[14], line 12\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mDataset já existente: \u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;241m+\u001b[39m nome_dataset)\n\u001b[1;32m     11\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 12\u001b[0m     df \u001b[38;5;241m=\u001b[39m \u001b[43mcriar_dataset\u001b[49m\u001b[43m(\u001b[49m\u001b[43morgao\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1000\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     13\u001b[0m     df\u001b[38;5;241m.\u001b[39mreset_index(drop\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, inplace\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m     14\u001b[0m     salvar_dataset(df, \u001b[38;5;28mstr\u001b[39m(orgao), \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mJE\u001b[39m\u001b[38;5;124m'\u001b[39m, data)\n",
            "Cell \u001b[0;32mIn[12], line 85\u001b[0m, in \u001b[0;36mcriar_dataset\u001b[0;34m(tribunal, data, tamanho_consulta, grau)\u001b[0m\n\u001b[1;32m     83\u001b[0m dados_dict \u001b[38;5;241m=\u001b[39m response\u001b[38;5;241m.\u001b[39mjson() \u001b[38;5;66;03m# <class 'dict'>\u001b[39;00m\n\u001b[1;32m     84\u001b[0m numero_processos \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(dados_dict[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhits\u001b[39m\u001b[38;5;124m'\u001b[39m][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhits\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[0;32m---> 85\u001b[0m ultima_posicao_dicionario \u001b[38;5;241m=\u001b[39m \u001b[43mdados_dict\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mhits\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mhits\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mdados_dict\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mhits\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mhits\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msort\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[1;32m     86\u001b[0m df_tjrn \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mconcat([df_tjrn, lista_para_dataframe(dados_dict)])\n\u001b[1;32m     87\u001b[0m df_tjrn \u001b[38;5;241m=\u001b[39m df_tjrn[df_tjrn[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mgrau\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m==\u001b[39m grau]\n",
            "\u001b[0;31mIndexError\u001b[0m: list index out of range"
          ]
        }
      ],
      "source": [
        "#lista_orgaos = ['2º JUIZADO ESPECIAL DA FAZENDA PÚBLICA', '3º JUIZADO ESPECIAL DA FAZENDA PÚBLICA', \n",
        "#                '5º JUIZADO ESPECIAL DA FAZENDA PÚBLICA']\n",
        "lista_tribunais = ['TRF1', 'TRF2', 'TRF3', 'TRF4']\n",
        "data = '2023-06-01'\n",
>>>>>>> ec389c935dbfa3937d63784935b9a2cb8b7a813e
        "for orgao in lista_tribunais:\n",
        "    nome_dataset = nome_df = str(orgao) + '_' + 'JE' + '_' + data + '.csv'\n",
        "    nome_dataset = nome_dataset.replace(\" \", \"_\")\n",
        "    nome_dataset = 'dados/' + nome_dataset\n",
        "    if os.path.isfile(nome_dataset):\n",
        "        print('Dataset já existente: ' + nome_dataset)\n",
        "    else:\n",
<<<<<<< HEAD
        "        df = criar_dataset(orgao, data, 5000)\n",
=======
        "        df = criar_dataset(orgao, data, 1000)\n",
>>>>>>> ec389c935dbfa3937d63784935b9a2cb8b7a813e
        "        df.reset_index(drop=True, inplace=True)\n",
        "        salvar_dataset(df, str(orgao), 'JE', data)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oikHltKLyjX0",
        "outputId": "3eb7cf67-b513-42ce-be88-783f27008d72"
      },
      "outputs": [],
      "source": [
        "print((df['orgao_julgador'][0]))"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "env",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
<<<<<<< HEAD
      "version": "3.10.6"
=======
      "version": "3.12.3"
>>>>>>> ec389c935dbfa3937d63784935b9a2cb8b7a813e
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
